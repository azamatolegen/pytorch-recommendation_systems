{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Factorization Machines (DeepFM).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-MKRhCyaxNU"
      },
      "source": [
        "# Deep Factorization Machines\r\n",
        "\r\n",
        "For real-world data where inherent feature crossing structures are usually very complex and nonlinear, second-order feature interactions  generally used in factorization machines in practice are often insufficient. Modeling higher degrees of feature combinations with factorization machines is possible theoretically but it is usually not adopted due to numerical instability and high computational complexity. One effective solution is using deep neural networks. \r\n",
        "\r\n",
        "Deep neural networks are powerful in feature representation learning and have the potential to learn sophisticated feature interactions. As such, it is natural to integrate deep neural networks to factorization machines. Adding nonlinear transformation layers to factorization machines gives it the capability to model both low-order feature combinations and high-order feature combinations. Moreover, non-linear inherent structures from inputs can also be captured with deep neural networks. As such, we will train a representative model named deep factorization machines (DeepFM) [Guo et al., 2017] which combine FM and deep neural networks.\r\n",
        "\r\n",
        "DeepFM consists of an FM component and a deep component which are integrated in a parallel structure. The FM component is the same as the 2-way factorization machines which is used to model the low-order feature interactions. The deep component is a multi-layered perceptron that is used to capture high-order feature interactions and nonlinearities. These two components share the same inputs/embeddings and their outputs are summed up as the final prediction. It is worth pointing out that the spirit of DeepFM resembles that of the Wide & Deep architecture which can capture both memorization and generalization. The advantages of DeepFM over the Wide & Deep model is that it reduces the effort of hand-crafted feature engineering by identifying feature combinations automatically.\r\n",
        "\r\n",
        "![](https://drive.google.com/uc?id=1KXC_8TRNC5Dj1w_NyDfyagzxAnbQDABb)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "* ntegrating neural networks to FM enables it to model complex and high-order \r\n",
        "interactions.\r\n",
        "\r\n",
        "* DeepFM outperforms the original FM on the advertising dataset.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rrT4wFPfn13"
      },
      "source": [
        "# Model implementation in PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEtvOeO2fo-0"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import tqdm\r\n",
        "from sklearn.metrics import roc_auc_score\r\n",
        "\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpvQfBdWha-v"
      },
      "source": [
        "class MovieLensDataset(Dataset):\r\n",
        "    \"\"\"\r\n",
        "        MovieLens 1M Dataset\r\n",
        "        Data preparation: treat samples with a rating less than 3 as negative samples\r\n",
        "        :param dataset_path: MovieLens dataset path\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(self, dataset_path, sep='::', engine='python', header=None):\r\n",
        "        # Read the data into a Pandas dataframe\r\n",
        "        data = pd.read_csv(dataset_path, sep=sep, engine=engine, header=header).to_numpy()[:, :3]\r\n",
        "\r\n",
        "        # Retrieve the items and ratings data\r\n",
        "        self.items = data[:, :2].astype(np.int) - 1  # -1 because ID begins from 1\r\n",
        "        self.targets = self.__preprocess_target(data[:, 2]).astype(np.float32)\r\n",
        "\r\n",
        "        # Get the range of the items\r\n",
        "        self.field_dims = np.max(self.items, axis=0) + 1\r\n",
        "\r\n",
        "        # Initialize NumPy arrays to store user and item indices\r\n",
        "        self.user_field_idx = np.array((0,), dtype=np.long)\r\n",
        "        self.item_field_idx = np.array((1,), dtype=np.long)\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        \"\"\"\r\n",
        "        :return: number of total ratings\r\n",
        "        \"\"\"\r\n",
        "        return self.targets.shape[0]\r\n",
        "\r\n",
        "    def __getitem__(self, index):\r\n",
        "        \"\"\"\r\n",
        "        :param index: current index\r\n",
        "        :return: the items and ratings at current index\r\n",
        "        \"\"\"\r\n",
        "        return self.items[index], self.targets[index]\r\n",
        "\r\n",
        "    def __preprocess_target(self, target):\r\n",
        "        \"\"\"\r\n",
        "        Preprocess the ratings into negative and positive samples\r\n",
        "        :param target: ratings\r\n",
        "        :return: binary ratings (0 or 1)\r\n",
        "        \"\"\"\r\n",
        "        target[target <= 3] = 0  # ratings less than or equal to 3 classified as 0\r\n",
        "        target[target > 3] = 1  # ratings bigger than 3 classified as 1\r\n",
        "        return target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMRAINuohbBU"
      },
      "source": [
        "class DeepFM(nn.Module):\r\n",
        "  \"\"\"\r\n",
        "  A Pytorch implementation of Deep Factorization Model\r\n",
        "  \"\"\"\r\n",
        "\r\n",
        "  def __init__(self, field_dims, embed_dim, mlp_dims, dropout):\r\n",
        "    super(DeepFM, self).__init__()\r\n",
        "    self.linear = FeaturesLinear(field_dims)\r\n",
        "    self.fm = FactorizationMachine(reduce_sum=True)\r\n",
        "    self.embedding = FeaturesEmbedding(field_dims, embed_dim)\r\n",
        "    self.embed_output_dim = len(field_dims) * embed_dim\r\n",
        "    self.mlp = MultiLayerPerceptron(self.embed_output_dim, mlp_dims, dropout)\r\n",
        "\r\n",
        "  def forward(self, x):\r\n",
        "    \"\"\"\r\n",
        "    :param x: Long tensor of size (batch_size, num_fields)\r\n",
        "    \"\"\"\r\n",
        "    embed_x = self.embedding(x)\r\n",
        "    x = self.linear(x) + self.fm(embed_x) + self.mlp(embed_x.view(-1, self.embed_output_dim))\r\n",
        "    return torch.sigmoid(x.squeeze(1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IBUI7MMn34w"
      },
      "source": [
        "class FeaturesLinear(torch.nn.Module):\r\n",
        "    \"\"\"\r\n",
        "    Class to perform a linear transformation on the features\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(self, field_dims, output_dim=1):\r\n",
        "        super().__init__()\r\n",
        "        self.fc = torch.nn.Embedding(sum(field_dims), output_dim)\r\n",
        "        self.bias = torch.nn.Parameter(torch.zeros((output_dim,)))\r\n",
        "        self.offsets = np.array((0, *np.cumsum(field_dims)[:-1]), dtype=np.long)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        \"\"\"\r\n",
        "        :param x: Long tensor of size ``(batch_size, num_fields)``\r\n",
        "        \"\"\"\r\n",
        "        x = x + x.new_tensor(self.offsets).unsqueeze(0)\r\n",
        "        return torch.sum(self.fc(x), dim=1) + self.bias\r\n",
        "\r\n",
        "\r\n",
        "class FeaturesEmbedding(torch.nn.Module):\r\n",
        "    \"\"\"\r\n",
        "    Class to get feature embeddings\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(self, field_dims, embed_dim):\r\n",
        "        super().__init__()\r\n",
        "        self.embedding = torch.nn.Embedding(sum(field_dims), embed_dim)\r\n",
        "        self.offsets = np.array((0, *np.cumsum(field_dims)[:-1]), dtype=np.long)\r\n",
        "        torch.nn.init.xavier_uniform_(self.embedding.weight.data)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        \"\"\"\r\n",
        "        :param x: Long tensor of size ``(batch_size, num_fields)``\r\n",
        "        \"\"\"\r\n",
        "        x = x + x.new_tensor(self.offsets).unsqueeze(0)\r\n",
        "        return self.embedding(x)\r\n",
        "\r\n",
        "\r\n",
        "class MultiLayerPerceptron(torch.nn.Module):\r\n",
        "    \"\"\"\r\n",
        "    Class to instantiate a Multilayer Perceptron model\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(self, input_dim, embed_dims, dropout, output_layer=True):\r\n",
        "        super().__init__()\r\n",
        "        layers = list()\r\n",
        "        for embed_dim in embed_dims:\r\n",
        "            layers.append(torch.nn.Linear(input_dim, embed_dim))\r\n",
        "            layers.append(torch.nn.BatchNorm1d(embed_dim))\r\n",
        "            layers.append(torch.nn.ReLU())\r\n",
        "            layers.append(torch.nn.Dropout(p=dropout))\r\n",
        "            input_dim = embed_dim\r\n",
        "        if output_layer:\r\n",
        "            layers.append(torch.nn.Linear(input_dim, 1))\r\n",
        "        self.mlp = torch.nn.Sequential(*layers)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        \"\"\"\r\n",
        "        :param x: Float tensor of size ``(batch_size, num_fields, embed_dim)``\r\n",
        "        \"\"\"\r\n",
        "        return self.mlp(x)\r\n",
        "\r\n",
        "\r\n",
        "class FactorizationMachine(torch.nn.Module):\r\n",
        "    \"\"\"\r\n",
        "        Class to instantiate a Factorization Machine model\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(self, reduce_sum=True):\r\n",
        "        super().__init__()\r\n",
        "        self.reduce_sum = reduce_sum\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        \"\"\"\r\n",
        "        :param x: Float tensor of size ``(batch_size, num_fields, embed_dim)``\r\n",
        "        \"\"\"\r\n",
        "        square_of_sum = torch.sum(x, dim=1) ** 2\r\n",
        "        sum_of_square = torch.sum(x ** 2, dim=1)\r\n",
        "        ix = square_of_sum - sum_of_square\r\n",
        "        if self.reduce_sum:\r\n",
        "            ix = torch.sum(ix, dim=1, keepdim=True)\r\n",
        "        return 0.5 * ix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJlQjuVohbJO"
      },
      "source": [
        "def fit(model, optimizer, data_loader, criterion, device, log_interval=1000):\r\n",
        "    \"\"\"\r\n",
        "    Train the model\r\n",
        "    :param model: choice of model\r\n",
        "    :param optimizer: choice of optimizer\r\n",
        "    :param data_loader: data loader class\r\n",
        "    :param criterion: choice of loss function\r\n",
        "    :param device: choice of device\r\n",
        "    :return: loss being logged\r\n",
        "    \"\"\"\r\n",
        "    # Step into train mode\r\n",
        "    model.train()\r\n",
        "    total_loss = 0\r\n",
        "    for i, (fields, target) in enumerate(tqdm.tqdm(data_loader, smoothing=0, mininterval=1.0)):\r\n",
        "        fields, target = fields.to(device), target.to(device)\r\n",
        "        y = model(fields)\r\n",
        "        loss = criterion(y, target.float())\r\n",
        "        model.zero_grad()\r\n",
        "        loss.backward()\r\n",
        "        optimizer.step()\r\n",
        "        total_loss += loss.item()\r\n",
        "\r\n",
        "        # Log the total loss for every 1000 runs\r\n",
        "        if (i + 1) % log_interval == 0:\r\n",
        "            print('    - loss:', total_loss / log_interval)\r\n",
        "            total_loss = 0\r\n",
        "\r\n",
        "def test(model, data_loader, device):\r\n",
        "    \"\"\"\r\n",
        "    Evaluate the model\r\n",
        "    :param model: choice of model\r\n",
        "    :param data_loader: data loader class\r\n",
        "    :param device: choice of device\r\n",
        "    :return: AUC score\r\n",
        "    \"\"\"\r\n",
        "    # Step into evaluation mode\r\n",
        "    model.eval()\r\n",
        "    targets, predicts = list(), list()\r\n",
        "    with torch.no_grad():\r\n",
        "        for fields, target in tqdm.tqdm(data_loader, smoothing=0, mininterval=1.0):\r\n",
        "            fields, target = fields.to(device), target.to(device)\r\n",
        "            y = model(fields)\r\n",
        "            targets.extend(target.tolist())\r\n",
        "            predicts.extend(y.tolist())\r\n",
        "\r\n",
        "    # Return AUC score between predicted ratings and actual ratings\r\n",
        "    return roc_auc_score(targets, predicts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CVt7V-Amqjxz",
        "outputId": "99c13cf0-4c78-4a72-f7a8-f03353e240f3"
      },
      "source": [
        "# get the data\r\n",
        "!wget http://files.grouplens.org/datasets/movielens/ml-1m.zip\r\n",
        "!unzip ml-1m.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-09 06:12:18--  http://files.grouplens.org/datasets/movielens/ml-1m.zip\n",
            "Resolving files.grouplens.org (files.grouplens.org)... 128.101.65.152\n",
            "Connecting to files.grouplens.org (files.grouplens.org)|128.101.65.152|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5917549 (5.6M) [application/zip]\n",
            "Saving to: ‘ml-1m.zip’\n",
            "\n",
            "ml-1m.zip           100%[===================>]   5.64M  27.2MB/s    in 0.2s    \n",
            "\n",
            "2021-02-09 06:12:18 (27.2 MB/s) - ‘ml-1m.zip’ saved [5917549/5917549]\n",
            "\n",
            "Archive:  ml-1m.zip\n",
            "   creating: ml-1m/\n",
            "  inflating: ml-1m/movies.dat        \n",
            "  inflating: ml-1m/ratings.dat       \n",
            "  inflating: ml-1m/README            \n",
            "  inflating: ml-1m/users.dat         \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2JJParYhbL8"
      },
      "source": [
        "# Get the dataset\r\n",
        "dataset = MovieLensDataset('./ml-1m/ratings.dat')\r\n",
        "\r\n",
        "# Split the data into 80% train, 10% validation, and 10% test\r\n",
        "train_length = int(len(dataset) * 0.8)\r\n",
        "valid_length = int(len(dataset) * 0.1)\r\n",
        "test_length = len(dataset) - train_length - valid_length\r\n",
        "train_dataset, valid_dataset, test_dataset = torch.utils.data.random_split(dataset, (train_length, valid_length, test_length))\r\n",
        "\r\n",
        "# Instantiate data loader classes for train, validation, and test sets\r\n",
        "train_data_loader = DataLoader(train_dataset, batch_size=1024, num_workers=8)\r\n",
        "valid_data_loader = DataLoader(valid_dataset, batch_size=1024, num_workers=8)\r\n",
        "test_data_loader = DataLoader(test_dataset, batch_size=1024, num_workers=8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmBYa9qWq929",
        "outputId": "dc2ca957-29d0-486a-f0a5-ea8bd34fea3a"
      },
      "source": [
        "# Get the model\r\n",
        "field_dims = dataset.field_dims\r\n",
        "learning_rate=0.001\r\n",
        "weight_decay=1e-6\r\n",
        "epoch=10\r\n",
        "device='cpu'\r\n",
        "\r\n",
        "model = DeepFM(field_dims, embed_dim=16, mlp_dims=(16, 16), dropout=0.5)\r\n",
        "# Use binary cross entropy loss\r\n",
        "criterion = torch.nn.BCELoss()\r\n",
        "# Use Adam optimizer\r\n",
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate, weight_decay=weight_decay)\r\n",
        "\r\n",
        "# Loop through pre-defined number of epochs\r\n",
        "for epoch_i in range(epoch):\r\n",
        "    # Perform training on the train set\r\n",
        "    fit(model, optimizer, train_data_loader, criterion, device)\r\n",
        "    # Perform evaluation on the validation set\r\n",
        "    valid_auc = test(model, valid_data_loader, device)\r\n",
        "    # Log the epochs and AUC on the validation set\r\n",
        "    print('epoch:', epoch_i, 'validation: auc:', valid_auc)\r\n",
        "\r\n",
        "# Perform evaluation on the test set\r\n",
        "test_auc = test(model, test_data_loader, device)\r\n",
        "# Log the final AUC on the test set\r\n",
        "print('test auc:', test_auc)\r\n",
        "\r\n",
        "# Save the model checkpoint\r\n",
        "torch.save(model.state_dict(), 'deepfm.pt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "  0%|          | 0/782 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "  5%|▍         | 36/782 [00:01<00:20, 35.65it/s]\u001b[A\u001b[A\n",
            "\n",
            " 12%|█▏        | 97/782 [00:02<00:14, 48.24it/s]\u001b[A\u001b[A\n",
            "\n",
            " 20%|██        | 159/782 [00:03<00:11, 52.68it/s]\u001b[A\u001b[A\n",
            "\n",
            " 28%|██▊       | 221/782 [00:04<00:10, 52.88it/s]\u001b[A\u001b[A\n",
            "\n",
            " 36%|███▌      | 283/782 [00:05<00:09, 54.54it/s]\u001b[A\u001b[A\n",
            "\n",
            " 44%|████▍     | 345/782 [00:06<00:07, 55.60it/s]\u001b[A\u001b[A\n",
            "\n",
            " 52%|█████▏    | 407/782 [00:07<00:06, 56.40it/s]\u001b[A\u001b[A\n",
            "\n",
            " 60%|█████▉    | 469/782 [00:08<00:05, 56.97it/s]\u001b[A\u001b[A\n",
            "\n",
            " 68%|██████▊   | 531/782 [00:09<00:04, 57.38it/s]\u001b[A\u001b[A\n",
            "\n",
            " 76%|███████▌  | 593/782 [00:10<00:03, 57.79it/s]\u001b[A\u001b[A\n",
            "\n",
            " 84%|████████▍ | 655/782 [00:11<00:02, 58.00it/s]\u001b[A\u001b[A\n",
            "\n",
            "100%|██████████| 782/782 [00:13<00:00, 58.17it/s]\n",
            "\n",
            "\n",
            "100%|██████████| 98/98 [00:01<00:00, 95.55it/s]\n",
            "\n",
            "\n",
            "  0%|          | 0/782 [00:00<?, ?it/s]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 0 validation: auc: 0.7716942366617788\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "  5%|▍         | 38/782 [00:01<00:19, 37.51it/s]\u001b[A\u001b[A\n",
            "\n",
            " 13%|█▎        | 98/782 [00:02<00:14, 48.53it/s]\u001b[A\u001b[A\n",
            "\n",
            " 20%|██        | 158/782 [00:03<00:12, 51.96it/s]\u001b[A\u001b[A\n",
            "\n",
            " 28%|██▊       | 218/782 [00:04<00:10, 53.88it/s]\u001b[A\u001b[A\n",
            "\n",
            " 36%|███▌      | 278/782 [00:05<00:09, 54.97it/s]\u001b[A\u001b[A\n",
            "\n",
            " 43%|████▎     | 339/782 [00:06<00:07, 55.95it/s]\u001b[A\u001b[A\n",
            "\n",
            " 51%|█████     | 400/782 [00:07<00:06, 56.55it/s]\u001b[A\u001b[A\n",
            "\n",
            " 59%|█████▉    | 461/782 [00:08<00:05, 57.00it/s]\u001b[A\u001b[A\n",
            "\n",
            " 67%|██████▋   | 522/782 [00:09<00:04, 56.94it/s]\u001b[A\u001b[A\n",
            "\n",
            " 75%|███████▍  | 583/782 [00:10<00:03, 56.35it/s]\u001b[A\u001b[A\n",
            "\n",
            " 82%|████████▏ | 644/782 [00:11<00:02, 56.64it/s]\u001b[A\u001b[A\n",
            "\n",
            " 90%|█████████ | 705/782 [00:12<00:01, 56.86it/s]\u001b[A\u001b[A\n",
            "\n",
            "100%|██████████| 782/782 [00:13<00:00, 56.77it/s]\n",
            "\n",
            "\n",
            "100%|██████████| 98/98 [00:01<00:00, 95.91it/s]\n",
            "\n",
            "\n",
            "  0%|          | 0/782 [00:00<?, ?it/s]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 1 validation: auc: 0.778057202956051\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "  5%|▌         | 40/782 [00:01<00:18, 39.30it/s]\u001b[A\u001b[A\n",
            "\n",
            " 13%|█▎        | 100/782 [00:02<00:13, 49.29it/s]\u001b[A\u001b[A\n",
            "\n",
            " 21%|██        | 161/782 [00:03<00:11, 53.09it/s]\u001b[A\u001b[A\n",
            "\n",
            " 28%|██▊       | 222/782 [00:04<00:10, 54.81it/s]\u001b[A\u001b[A\n",
            "\n",
            " 36%|███▌      | 283/782 [00:05<00:08, 55.88it/s]\u001b[A\u001b[A\n",
            "\n",
            " 44%|████▍     | 344/782 [00:06<00:07, 56.42it/s]\u001b[A\u001b[A\n",
            "\n",
            " 52%|█████▏    | 405/782 [00:07<00:06, 56.90it/s]\u001b[A\u001b[A\n",
            "\n",
            " 60%|█████▉    | 466/782 [00:08<00:05, 57.34it/s]\u001b[A\u001b[A\n",
            "\n",
            " 67%|██████▋   | 527/782 [00:09<00:04, 57.71it/s]\u001b[A\u001b[A\n",
            "\n",
            " 75%|███████▌  | 588/782 [00:10<00:03, 57.95it/s]\u001b[A\u001b[A\n",
            "\n",
            " 83%|████████▎ | 649/782 [00:11<00:02, 58.21it/s]\u001b[A\u001b[A\n",
            "\n",
            " 91%|█████████ | 710/782 [00:12<00:01, 58.40it/s]\u001b[A\u001b[A\n",
            "\n",
            "100%|██████████| 782/782 [00:13<00:00, 58.18it/s]\n",
            "\n",
            "\n",
            "100%|██████████| 98/98 [00:01<00:00, 95.07it/s]\n",
            "\n",
            "\n",
            "  0%|          | 0/782 [00:00<?, ?it/s]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 2 validation: auc: 0.7817825948637652\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "  5%|▍         | 39/782 [00:01<00:19, 38.41it/s]\u001b[A\u001b[A\n",
            "\n",
            " 13%|█▎        | 100/782 [00:02<00:13, 49.49it/s]\u001b[A\u001b[A\n",
            "\n",
            " 21%|██        | 161/782 [00:03<00:11, 52.27it/s]\u001b[A\u001b[A\n",
            "\n",
            " 28%|██▊       | 222/782 [00:04<00:10, 53.33it/s]\u001b[A\u001b[A\n",
            "\n",
            " 36%|███▌      | 283/782 [00:05<00:09, 54.42it/s]\u001b[A\u001b[A\n",
            "\n",
            " 44%|████▍     | 344/782 [00:06<00:07, 55.29it/s]\u001b[A\u001b[A\n",
            "\n",
            " 52%|█████▏    | 405/782 [00:07<00:06, 55.81it/s]\u001b[A\u001b[A\n",
            "\n",
            " 60%|█████▉    | 466/782 [00:08<00:05, 56.22it/s]\u001b[A\u001b[A\n",
            "\n",
            " 67%|██████▋   | 527/782 [00:09<00:04, 56.51it/s]\u001b[A\u001b[A\n",
            "\n",
            " 75%|███████▌  | 588/782 [00:10<00:03, 56.77it/s]\u001b[A\u001b[A\n",
            "\n",
            " 83%|████████▎ | 649/782 [00:11<00:02, 57.04it/s]\u001b[A\u001b[A\n",
            "\n",
            " 91%|█████████ | 710/782 [00:12<00:01, 57.25it/s]\u001b[A\u001b[A\n",
            "\n",
            "100%|██████████| 782/782 [00:13<00:00, 56.92it/s]\n",
            "\n",
            "\n",
            "100%|██████████| 98/98 [00:01<00:00, 95.03it/s]\n",
            "\n",
            "\n",
            "  0%|          | 0/782 [00:00<?, ?it/s]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 3 validation: auc: 0.7823289699267744\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "  5%|▍         | 38/782 [00:01<00:19, 37.91it/s]\u001b[A\u001b[A\n",
            "\n",
            " 12%|█▏        | 97/782 [00:02<00:14, 48.08it/s]\u001b[A\u001b[A\n",
            "\n",
            " 20%|█▉        | 156/782 [00:03<00:12, 51.23it/s]\u001b[A\u001b[A\n",
            "\n",
            " 27%|██▋       | 215/782 [00:04<00:10, 52.66it/s]\u001b[A\u001b[A\n",
            "\n",
            " 35%|███▌      | 274/782 [00:05<00:09, 52.20it/s]\u001b[A\u001b[A\n",
            "\n",
            " 43%|████▎     | 334/782 [00:06<00:08, 53.36it/s]\u001b[A\u001b[A\n",
            "\n",
            " 50%|█████     | 394/782 [00:07<00:07, 54.20it/s]\u001b[A\u001b[A\n",
            "\n",
            " 58%|█████▊    | 454/782 [00:08<00:05, 54.85it/s]\u001b[A\u001b[A\n",
            "\n",
            " 66%|██████▌   | 514/782 [00:09<00:04, 55.32it/s]\u001b[A\u001b[A\n",
            "\n",
            " 73%|███████▎  | 574/782 [00:10<00:03, 55.71it/s]\u001b[A\u001b[A\n",
            "\n",
            " 81%|████████  | 634/782 [00:11<00:02, 56.01it/s]\u001b[A\u001b[A\n",
            "\n",
            " 89%|████████▊ | 694/782 [00:12<00:01, 56.30it/s]\u001b[A\u001b[A\n",
            "\n",
            "100%|██████████| 782/782 [00:13<00:00, 56.32it/s]\n",
            "\n",
            "\n",
            "100%|██████████| 98/98 [00:01<00:00, 91.51it/s]\n",
            "\n",
            "\n",
            "  0%|          | 0/782 [00:00<?, ?it/s]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 4 validation: auc: 0.7844714420679844\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "  5%|▌         | 40/782 [00:01<00:18, 39.42it/s]\u001b[A\u001b[A\n",
            "\n",
            " 13%|█▎        | 101/782 [00:02<00:13, 49.94it/s]\u001b[A\u001b[A\n",
            "\n",
            " 21%|██        | 162/782 [00:03<00:11, 53.31it/s]\u001b[A\u001b[A\n",
            "\n",
            " 29%|██▊       | 223/782 [00:04<00:10, 54.87it/s]\u001b[A\u001b[A\n",
            "\n",
            " 36%|███▋      | 284/782 [00:05<00:08, 55.95it/s]\u001b[A\u001b[A\n",
            "\n",
            " 44%|████▍     | 345/782 [00:06<00:07, 56.67it/s]\u001b[A\u001b[A\n",
            "\n",
            " 52%|█████▏    | 406/782 [00:07<00:06, 56.68it/s]\u001b[A\u001b[A\n",
            "\n",
            " 60%|█████▉    | 467/782 [00:08<00:05, 57.00it/s]\u001b[A\u001b[A\n",
            "\n",
            " 68%|██████▊   | 528/782 [00:09<00:04, 57.30it/s]\u001b[A\u001b[A\n",
            "\n",
            " 75%|███████▌  | 589/782 [00:10<00:03, 57.49it/s]\u001b[A\u001b[A\n",
            "\n",
            " 83%|████████▎ | 650/782 [00:11<00:02, 57.74it/s]\u001b[A\u001b[A\n",
            "\n",
            " 91%|█████████ | 711/782 [00:12<00:01, 57.92it/s]\u001b[A\u001b[A\n",
            "\n",
            "100%|██████████| 782/782 [00:13<00:00, 57.72it/s]\n",
            "\n",
            "\n",
            "100%|██████████| 98/98 [00:01<00:00, 96.51it/s]\n",
            "\n",
            "\n",
            "  0%|          | 0/782 [00:00<?, ?it/s]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 5 validation: auc: 0.7857398733393367\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "  5%|▌         | 40/782 [00:01<00:18, 39.36it/s]\u001b[A\u001b[A\n",
            "\n",
            " 13%|█▎        | 101/782 [00:02<00:13, 49.72it/s]\u001b[A\u001b[A\n",
            "\n",
            " 21%|██        | 162/782 [00:03<00:11, 53.16it/s]\u001b[A\u001b[A\n",
            "\n",
            " 29%|██▊       | 223/782 [00:04<00:10, 55.00it/s]\u001b[A\u001b[A\n",
            "\n",
            " 36%|███▋      | 284/782 [00:05<00:08, 55.90it/s]\u001b[A\u001b[A\n",
            "\n",
            " 44%|████▍     | 345/782 [00:06<00:07, 56.55it/s]\u001b[A\u001b[A\n",
            "\n",
            " 52%|█████▏    | 406/782 [00:07<00:06, 57.03it/s]\u001b[A\u001b[A\n",
            "\n",
            " 60%|█████▉    | 467/782 [00:08<00:05, 57.36it/s]\u001b[A\u001b[A\n",
            "\n",
            " 68%|██████▊   | 528/782 [00:09<00:04, 57.68it/s]\u001b[A\u001b[A\n",
            "\n",
            " 75%|███████▌  | 589/782 [00:10<00:03, 57.89it/s]\u001b[A\u001b[A\n",
            "\n",
            " 83%|████████▎ | 650/782 [00:11<00:02, 58.06it/s]\u001b[A\u001b[A\n",
            "\n",
            " 91%|█████████ | 711/782 [00:12<00:01, 58.25it/s]\u001b[A\u001b[A\n",
            "\n",
            "100%|██████████| 782/782 [00:13<00:00, 58.07it/s]\n",
            "\n",
            "\n",
            "100%|██████████| 98/98 [00:01<00:00, 96.56it/s]\n",
            "\n",
            "\n",
            "  0%|          | 0/782 [00:00<?, ?it/s]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 6 validation: auc: 0.7858337378842934\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "  5%|▍         | 39/782 [00:01<00:19, 38.78it/s]\u001b[A\u001b[A\n",
            "\n",
            " 13%|█▎        | 98/782 [00:02<00:14, 48.80it/s]\u001b[A\u001b[A\n",
            "\n",
            " 20%|██        | 159/782 [00:03<00:11, 52.66it/s]\u001b[A\u001b[A\n",
            "\n",
            " 28%|██▊       | 220/782 [00:04<00:10, 53.67it/s]\u001b[A\u001b[A\n",
            "\n",
            " 36%|███▌      | 281/782 [00:05<00:09, 54.82it/s]\u001b[A\u001b[A\n",
            "\n",
            " 44%|████▎     | 342/782 [00:06<00:07, 55.61it/s]\u001b[A\u001b[A\n",
            "\n",
            " 52%|█████▏    | 403/782 [00:07<00:06, 56.21it/s]\u001b[A\u001b[A\n",
            "\n",
            " 59%|█████▉    | 464/782 [00:08<00:05, 56.78it/s]\u001b[A\u001b[A\n",
            "\n",
            " 67%|██████▋   | 525/782 [00:09<00:04, 57.15it/s]\u001b[A\u001b[A\n",
            "\n",
            " 75%|███████▍  | 586/782 [00:10<00:03, 57.38it/s]\u001b[A\u001b[A\n",
            "\n",
            " 83%|████████▎ | 647/782 [00:11<00:02, 57.32it/s]\u001b[A\u001b[A\n",
            "\n",
            " 91%|█████████ | 708/782 [00:12<00:01, 57.43it/s]\u001b[A\u001b[A\n",
            "\n",
            "100%|██████████| 782/782 [00:13<00:00, 57.28it/s]\n",
            "\n",
            "\n",
            "100%|██████████| 98/98 [00:01<00:00, 94.96it/s]\n",
            "\n",
            "\n",
            "  0%|          | 0/782 [00:00<?, ?it/s]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 7 validation: auc: 0.7866237857522\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "  5%|▌         | 41/782 [00:01<00:18, 40.32it/s]\u001b[A\u001b[A\n",
            "\n",
            " 13%|█▎        | 101/782 [00:02<00:13, 49.96it/s]\u001b[A\u001b[A\n",
            "\n",
            " 21%|██        | 161/782 [00:03<00:11, 52.58it/s]\u001b[A\u001b[A\n",
            "\n",
            " 28%|██▊       | 222/782 [00:04<00:10, 54.45it/s]\u001b[A\u001b[A\n",
            "\n",
            " 36%|███▌      | 283/782 [00:05<00:08, 55.57it/s]\u001b[A\u001b[A\n",
            "\n",
            " 44%|████▍     | 344/782 [00:06<00:07, 55.24it/s]\u001b[A\u001b[A\n",
            "\n",
            " 52%|█████▏    | 405/782 [00:07<00:06, 55.96it/s]\u001b[A\u001b[A\n",
            "\n",
            " 60%|█████▉    | 466/782 [00:08<00:05, 56.46it/s]\u001b[A\u001b[A\n",
            "\n",
            " 67%|██████▋   | 527/782 [00:09<00:04, 56.86it/s]\u001b[A\u001b[A\n",
            "\n",
            " 75%|███████▌  | 588/782 [00:10<00:03, 57.23it/s]\u001b[A\u001b[A\n",
            "\n",
            " 83%|████████▎ | 649/782 [00:11<00:02, 57.46it/s]\u001b[A\u001b[A\n",
            "\n",
            " 91%|█████████ | 710/782 [00:12<00:01, 57.63it/s]\u001b[A\u001b[A\n",
            "\n",
            "100%|██████████| 782/782 [00:13<00:00, 57.49it/s]\n",
            "\n",
            "\n",
            "100%|██████████| 98/98 [00:01<00:00, 97.17it/s]\n",
            "\n",
            "\n",
            "  0%|          | 0/782 [00:00<?, ?it/s]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 8 validation: auc: 0.7879520626528944\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "  5%|▍         | 38/782 [00:01<00:19, 37.86it/s]\u001b[A\u001b[A\n",
            "\n",
            " 13%|█▎        | 98/782 [00:02<00:14, 48.64it/s]\u001b[A\u001b[A\n",
            "\n",
            " 20%|██        | 159/782 [00:03<00:11, 52.51it/s]\u001b[A\u001b[A\n",
            "\n",
            " 28%|██▊       | 220/782 [00:04<00:10, 54.26it/s]\u001b[A\u001b[A\n",
            "\n",
            " 36%|███▌      | 281/782 [00:05<00:09, 55.11it/s]\u001b[A\u001b[A\n",
            "\n",
            " 44%|████▎     | 342/782 [00:06<00:07, 55.68it/s]\u001b[A\u001b[A\n",
            "\n",
            " 52%|█████▏    | 403/782 [00:07<00:06, 56.08it/s]\u001b[A\u001b[A\n",
            "\n",
            " 59%|█████▉    | 464/782 [00:08<00:05, 56.35it/s]\u001b[A\u001b[A\n",
            "\n",
            " 67%|██████▋   | 525/782 [00:09<00:04, 56.60it/s]\u001b[A\u001b[A\n",
            "\n",
            " 75%|███████▍  | 586/782 [00:10<00:03, 56.76it/s]\u001b[A\u001b[A\n",
            "\n",
            " 83%|████████▎ | 647/782 [00:11<00:02, 56.95it/s]\u001b[A\u001b[A\n",
            "\n",
            " 91%|█████████ | 708/782 [00:12<00:01, 57.19it/s]\u001b[A\u001b[A\n",
            "\n",
            "100%|██████████| 782/782 [00:13<00:00, 56.82it/s]\n",
            "\n",
            "\n",
            "100%|██████████| 98/98 [00:01<00:00, 91.17it/s]\n",
            "\n",
            "\n",
            "  0%|          | 0/98 [00:00<?, ?it/s]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 9 validation: auc: 0.7886707771358512\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r100%|██████████| 98/98 [00:01<00:00, 93.95it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "test auc: 0.7920102605357593\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xh7WUSfR2DtX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}